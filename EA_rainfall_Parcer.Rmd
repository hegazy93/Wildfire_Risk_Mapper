---
title: "MetOffice DataPoint Parser"
author: "Abdelrahman Ibrahim"
date: "2023-11-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load essintial packages 

```{r, include=TRUE}
require(httr)
require(jsonlite)
require(sf)
require(tidyverse)
require(purrr)

```

# Build a database of daily rainfall amounts across all rainfall stations

## Get the names and locations of rainfall stations: 

```{r , include=TRUE}
stations <- readr::read_csv("http://environment.data.gov.uk/flood-monitoring/id/stations.csv", show_col_types = FALSE) %>% 
  filter(parameter == "rainfall")%>%
  filter(stationReference != 'Not_Specified') %>%
  select(stationReference, lat, long)
head(stations)
```

## Get the quarter hourly rainfall data for all stations and aggragate rainfall amounts per day:

```{r , include=FALSE }
#Initial call - Remove if(FALSE){} to run

if(FALSE){base_url <- 'https://environment.data.gov.uk/flood-monitoring/archive/readings-full-'
range <- seq(as.Date("2023/08/19"), as.Date("2023/11/22"), by = "days")
out_url <- apply(expand.grid(base_url, range, ".csv"), 1, paste, collapse = '')

rainfall_df <- readr::read_csv(out_url, col_select = c("stationReference","parameter", "value","date"), col_types = cols(stationReference = col_character(), date = col_date(), value =col_double())) %>%
  filter(parameter == "rainfall") %>%
  filter(stationReference != 'Not_Specified') %>%
  group_by(date, stationReference) %>%
  summarise(daily_rainfall = sum(value)) %>%
  full_join(stations, by = c("stationReference")) %>%
  drop_na()

head(rainfall_df)}

```

## Update the rainfall data to the most recent recording

```{r , include=TRUE}
base_url <- 'https://environment.data.gov.uk/flood-monitoring/archive/readings-full-'
range <- seq(as.Date("2023/02/01"), as.Date("2023/02/28"), by = "days")
out_url <- apply(expand.grid(base_url, range, ".csv"), 1, paste, collapse = '')

rainfall_df <- readr::read_csv(out_url, col_select = c("stationReference","parameter", "value","date"), col_types = cols(stationReference = col_character(), date = col_date(), value =col_double())) %>%
  filter(parameter == "rainfall") %>%
  filter(stationReference != 'Not_Specified') %>%
  group_by(date, stationReference) %>%
  summarise(daily_rainfall = sum(value)) %>%
  full_join(stations, by = c("stationReference")) %>%
  drop_na()

head(rainfall_df)
```


## Append the daily rainfall database

```{r , include=TRUE}

rainfall_hist <- readr::read_csv((r"(2_processed_data\EA_daily_rainfall_stations.csv)"), col_types = cols(stationReference = col_character(), date = col_date(), value =col_double()))
rainfall_master <- rbind(rainfall_hist, rainfall_df)
rainfall_master <-  unique(rainfall_master)
tail(rainfall_master)

write.table(rainfall_master, (r"(2_processed_data\EA_daily_rainfall_stations.csv)"), sep = ",", row.names = FALSE)

```

## Create a list of dataframes for each date


```{r , include=TRUE}
rainfall_list <- split(rainfall_df, f = rainfall_df$date)
rainfall_list[[1]]
```

# IDW Interpolation of daily rainfall value

```{r}
library(terra)
library(gstat)
library(tidyterra)
library(sp)
library(raster)
library(tmap)
```

## Build and IDW interpolation model for one day
```{r}
# 1. read in land cover data to define an extent and a clip box grid
gr = rast(r"(2_processed_data\landcover_MRSDNM.tif)", lyrs = 1)
plot(gr)
head(gr)
```

```{r}
# now define a clip extent + 20km
# and make a polygon to clip the data
ex = ext(gr)
fac = 20000
xmin = ex[1] - fac
xmax = ex[2] + fac
ymin = ex[3] - fac
ymax = ex[4] + fac
# define a closed box
my.box <- data.frame(
  ID = c(rep("1",5)),
  X = c(xmin, xmax, xmax, xmin, xmin),
  Y = c(ymin, ymin, ymax, ymax, ymin))
# my.poly
# then make polygon 
my.poly <- sfheaders::sf_polygon(
  obj = my.box, x = "X", y = "Y", polygon_id = "ID"
)
st_crs(my.poly) = 27700
plot(my.poly)

my.poly$ID <- as.numeric(as.character(my.poly$ID))
# make grid
rp <- rasterize(my.poly, gr, "ID")
head(rp)
```

```{r}
# 2. read in rainfall, make into a spatial object and create a surface 
r = rainfall_list[[1]]

# make spatial and convert to OSGB projection
r = r |>
  st_as_sf(coords = c("long", "lat"), crs =4326) |>
  st_transform(27700)

r <- r[my.poly,]

# view readings of the included stations
tmap_mode("view")
tm_shape(r) +
  tm_dots("daily_rainfall", 
              style="quantile", 
              title="Daily Rainfall")
```


```{r}
# 3. Build and fit the IDW model
fit_drain <- gstat(id = "daily_rainfall", formula = daily_rainfall~1, data=r, set=list(idp = 2))

interpolate_gstat <- function(model, x, crs, ...) {
	v <- st_as_sf(x, coords=c("x", "y"), crs=crs)
	p <- predict(model, v, ...)
	as.data.frame(p)[,1:2]
}

drain_int <- interpolate(rp, fit_drain, debug.level=0, fun=interpolate_gstat, crs=crs(r), index=1)

# Plot the results	
ggplot(my.poly) + geom_sf() +
	tidyterra::geom_spatraster(data = drain_int, aes(fill = daily_rainfall.pred)) + 
	scale_fill_viridis_c() +
	geom_sf(data = r)

```

## Iterate the IDW over the recorded days and save daily rasters separately:
```{r}
for (i in 1:length(rainfall_list)){
  r = rainfall_list[[i]]
  # make spatial and convert to OSGB projection
  r = r |>
    st_as_sf(coords = c("long", "lat"), crs =4326) |>
    st_transform(27700)
  r <- r[my.poly,]
  # 3. Build and fit the IDW model
  fit_drain <- gstat(id = "daily_rainfall", formula = daily_rainfall~1, data=r, set=list(idp = 2))
  
  interpolate_gstat <- function(model, x, crs, ...) {
    v <- st_as_sf(x, coords=c("x", "y"), crs=crs)
    p <- predict(model, v, ...)
    as.data.frame(p)[,1:2]
}
  drain_int <- interpolate(rp, fit_drain, debug.level=0, fun=interpolate_gstat, crs=crs(r), index=1)
   # Save the results to a tif images with the date as a filename
  output_file <- file.path(r"(2_processed_data\interpolated_rainfall)", paste0(as.character(names(rainfall_list[i])), ".tif"))
  writeRaster(drain_int, output_file, overwrite = TRUE)
}

```

## create a raster stack:

```{r}
create_raster_stack <- function(file_path) {
  # Get the list of TIFF files in the specified directory
  tif_files <- list.files(file_path, pattern = "\\.tif$", full.names = TRUE)
  # Sort the TIFF files based on their names
  tif_files <- sort(tif_files)
  # Create an empty raster stack object
  raster_stack <- stack()
  # Loop through the TIFF files and add them as layers to the raster stack
  for (i in 1:length(tif_files)) {
    # Read the TIFF file as a raster layer
    raster_layer <- raster(tif_files[i])
    # Extract the date from the TIFF file name
    date <- gsub(".tif", "", basename(tif_files[i]))
    # Set the layer name as the date
    names(raster_layer) <- date
    # Add the raster layer to the raster stack
    raster_stack <- addLayer(raster_stack, raster_layer)
  }
  
  # Return the raster stack
  return(raster_stack)
}

# Specify the path to the directory containing the TIFF files
file_path <- r"(2_processed_data\interpolated_rainfall)"
 
# Create and save raster stack
rainfall_stack <- create_raster_stack(file_path)
writeRaster(rainfall_stack, filename = r"(2_processed_data\daily_rainfall.tif)", format = "GTiff", overwrite=TRUE)
```

```{r}
rainfall_stack[[1]]
```


# Calculate Antecedent Rainfall Index for all days

```{r}

```





